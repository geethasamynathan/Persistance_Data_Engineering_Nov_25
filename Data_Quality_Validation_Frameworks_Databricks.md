
# â­ Data Quality & Validation Frameworks in Databricks

## ðŸ“˜ What is Data Quality?

Data Quality ensures that data entering the Databricks Lakehouse is:

âœ” Accurate  
âœ” Complete  
âœ” Consistent  
âœ” Valid  
âœ” Reliable  
âœ” Fresh  

Databricks provides multiple built-in and external frameworks to guarantee high-quality data pipelines across **Bronze â†’ Silver â†’ Gold** layers.

---

# â­ Why Data Quality Matters

Real-world data often contains:

- Missing values  
- Incorrect types  
- Duplicates  
- Invalid ranges  
- Schema drift  
- Corrupted files  

Without validation â†’ Bad data enters Silver/Gold layers â†’ Wrong dashboards â†’ Wrong business decisions.

---

# â­ Data Quality & Validation Frameworks in Databricks

There are **4 major approaches**:

---

# ðŸŸ£ 1. Delta Lake Data Quality (Built-In)

Delta Lake provides:

### âœ” Schema Enforcement  
Rejects data that doesnâ€™t match expected schema.

```python
df.write.format("delta").save(path)
```

### âœ” Schema Evolution  
Allows adding new columns safely.

```python
.option("mergeSchema", "true")
```

### âœ” Delta Constraints  
Enforce rules at the table level.

```sql
ALTER TABLE sales ADD CONSTRAINT valid_qty CHECK (quantity > 0);
```

---

# ðŸŸ£ 2. Delta Live Tables Expectations (Enterprise/Trial)

Delta Live Tables (DLT) provides:

âœ” Built-in validation  
âœ” Rule tagging  
âœ” Automatic bad record tracking  
âœ” Quality metrics  
âœ” Pipeline monitoring  

Example:

```python
@dlt.expect("valid_quantity", "quantity > 0")
def clean_sales():
    return dlt.read("raw_sales")
```

âš ï¸ *DLT is not available in Community Edition.*

---

# ðŸŸ£ 3. PySpark-Based Validation (Works in Community Edition)

Use PySpark logic to validate:

### Example:

```python
from pyspark.sql.functions import col

clean_df = raw_df     .filter(col("quantity") > 0)     .filter(col("price") > 0)     .filter(col("customer_id").isNotNull())
```

Capture bad rows:

```python
invalid_df = raw_df.filter(
    (col("quantity") <= 0) |
    (col("price") <= 0)
)
```

---

# ðŸŸ£ 4. Great Expectations (Open Source Framework)

Great Expectations integrates with Databricks for powerful validations.

Example:

```python
import great_expectations as gx
df_ge = gx.from_pandas(df.toPandas())

df_ge.expect_column_values_to_not_be_null("customer_id")
df_ge.expect_column_values_to_be_between("quantity", 1, 1000)
```

---

# â­ Types of Data Quality Checks

| Type | Examples |
|------|----------|
| **Completeness** | Null checks |
| **Validity** | Range checks |
| **Accuracy** | Negative numbers invalid |
| **Uniqueness** | Duplicate order IDs |
| **Freshness** | Timestamp checks |
| **Consistency** | Foreign key lookups |
| **Schema Rules** | Type enforcement |

---

# â­ Data Quality in the Medallion Architecture

## ðŸ¥‰ Bronze Layer
âœ” Preserve raw data  
âœ” Basic ingestion checks  
âœ” Capture corrupted rows  

## ðŸ¥ˆ Silver Layer
âœ” Deduplicate  
âœ” Validate values  
âœ” Join reference data  
âœ” Type casting  
âœ” Business rules  

## ðŸ¥‡ Gold Layer
âœ” Validated KPIs  
âœ” Financial checks  
âœ” Dashboard-ready metrics  

---

# â­ Real-World Example: Retail Pipeline

### Bronze â†’ Silver Validation Rules
- quantity > 0  
- price > 0  
- order_id must be numeric  
- product_id must exist  

### PySpark Example

```python
good_df = sales_df.filter(
    (col("order_id").isNotNull()) &
    (col("quantity") > 0) &
    (col("price") > 0)
)

bad_df = sales_df.filter(
    (col("order_id").isNull()) |
    (col("quantity") <= 0) |
    (col("price") <= 0)
)
```

Save valid data:

```python
good_df.write.format("delta").mode("overwrite").save("/dbfs/silver/sales")
```

Save bad data:

```python
bad_df.write.format("delta").mode("append").save("/dbfs/bronze/error_sales")
```

---

# â­ Summary

**Data Quality & Validation Frameworks in Databricks ensure that only clean, validated, consistent data moves from Bronze â†’ Silver â†’ Gold through built-in Delta Lake constraints, PySpark validation, DLT expectations, and Great Expectations.**

---

# ðŸŽ Need More?

I can generate:

âœ” HTML version  
âœ” DOCX version  
âœ” PPTX slides  
âœ” Databricks Notebook (.dbc)

Just ask!
