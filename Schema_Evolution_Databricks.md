
# â­ Schema Evolution in Databricks (Delta Lake)

## ğŸ“˜ What is Schema Evolution?

Schema Evolution refers to Delta Lakeâ€™s ability to **automatically adjust the table schema** when new columns or structural changes appear in incoming data.

It ensures your pipelines do NOT fail when:
- New columns are added
- Column order changes
- Nested fields expand
- Upstream systems evolve

---

## â­ Why Schema Evolution is Needed

Real-world data changes frequently:

### âœ” API updates introduce new fields  
### âœ” IoT devices send new attributes  
### âœ” Product catalog adds new metadata  
### âœ” CRM systems change structure  

Without schema evolution â†’ ETL breaks.

Delta Lake prevents this by allowing schema changes safely.

---

## â­ Schema Enforcement vs Schema Evolution

| Feature | Meaning |
|--------|---------|
| **Schema Enforcement** | Reject mismatched schemas |
| **Schema Evolution** | Allow safe schema adjustments |

Delta uses both:
- Enforcement â†’ protects correctness  
- Evolution â†’ supports flexibility  

---

# ğŸ§ª Practical Example (Databricks Community Edition)

## ğŸ“˜ Step 1 â€” Create original Delta table

```python
data1 = [
    (1, "Ram", 50000),
    (2, "Sita", 60000)
]

df1 = spark.createDataFrame(data1, ["id", "name", "salary"])

df1.write.format("delta").mode("overwrite").save("/dbfs/tmp/employees_schema")
```

---

## ğŸ“˜ Step 2 â€” New data arrives with new column

```python
data2 = [
    (1, "Ram", 50000, "IT"),
    (2, "Sita", 60000, "HR")
]

df2 = spark.createDataFrame(data2, ["id", "name", "salary", "department"])
```

Writing normally will fail with a schema mismatch.

---

## ğŸ“˜ Step 3 â€” Enable Schema Evolution

### âœ” PySpark

```python
df2.write   .option("mergeSchema", "true")   .format("delta")   .mode("append")   .save("/dbfs/tmp/employees_schema")
```

### âœ” SQL

```sql
ALTER TABLE delta.`/dbfs/tmp/employees_schema`
SET TBLPROPERTIES ('delta.mergeSchema' = 'true');
```

---

## ğŸ“˜ Step 4 â€” Read and verify

```python
display(spark.read.format("delta").load("/dbfs/tmp/employees_schema"))
```

Output now includes:

| id | name | salary | department |
|----|------|--------|------------|
| 1 | Ram | 50000 | IT |
| 2 | Sita | 60000 | HR |

---

# â­ Where Schema Evolution Is Useful

### âœ” IoT data streams  
### âœ” API ingestion  
### âœ” E-commerce product catalogs  
### âœ” CRM/ERP systems  
### âœ” Slowly evolving enterprise schemas  

---

# â­ When NOT to Use Schema Evolution

âŒ Banking or regulated environments  
âŒ When schema must be tightly controlled  
âŒ When incoming data may be messy  

Use **schema enforcement only** in such cases.

---

# â­ Summary

Schema Evolution in Delta Lake allows safe and flexible schema updatesâ€”including new columnsâ€”without breaking pipelines, while still maintaining data quality.

---

# ğŸ Need HTML / Word / PPT?

Tell me:
- **â€œexport to HTMLâ€**
- **â€œexport to DOCXâ€**
- **â€œexport to PPTXâ€**
