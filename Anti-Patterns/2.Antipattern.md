# ğŸ§© What Is an Anti-Pattern in Data Engineering (PySpark Context)?
## âœ… Definition
An anti-pattern is a **commonly used approach that looks correct initially but causes performance problems, higher cost, failures, and long-term maintenance issues**.

Slow Spark jobs

High cluster cost

Memory failures

Wrong results

Pipeline instability

Poor scalability

In Data Engineering, anti-patterns often appear when developers treat Spark like Python or SQL instead of a distributed computing engine.

ğŸ¯ Why Anti-Patterns Occur

Typical reasons:

Developers are new to distributed systems

Coming from pandas/Python background

Not understanding Spark execution (Catalyst optimizer, DAG, partitions, executors)

Rushing to finish tasks

Lack of data volume awareness

Poor architectural planning

ğŸ§  How to Avoid Anti-Patterns

Understand Sparkâ€™s distributed nature

Minimize data movement (shuffles, collects)

Use built-in Spark functions

Follow Medallion (Bronzeâ€“Silverâ€“Gold) architecture

Optimize partitions & file structure

Enable caching only when needed

Use Delta Lake features instead of manual workarounds

Avoid designing pipelines with hard-coded, unscalable logic

ğŸ”¥ List of All Possible Anti-Patterns in PySpark & Data Engineering (with Examples)

(Explained clearly & simply)

ğŸŸ¦ 1. Anti-Pattern: Using Python UDFs Instead of Spark Functions
âŒ Problem

Python UDFs execute outside the JVM â†’ slow, not optimized.

âŒ Example (Wrong Approach)
```python
from pyspark.sql.functions import udf

def add_tax(x):
    return x * 0.18

tax_udf = udf(add_tax)
df = df.withColumn("tax", tax_udf(df.amount))
```
â— Why This Is Bad

10Ã— slower

Cannot be optimized by Catalyst

Memory & serialization overhead

âœ… Correct Approach â€” Use Spark SQL Functions
```python
from pyspark.sql.functions import col

df = df.withColumn("tax", col("amount") * 0.18)
```
### ğŸ¯ How It Fixes the Problem

Spark can push-down transformations

Executes inside JVM (faster)

Better parallelization

### ğŸŸ§ 2. Anti-Pattern: Using collect() / toPandas() on Large Data
âŒ Problem

Brings entire DataFrame to driver â†’ memory crash.

âŒ Example (Wrong)
data = df.collect()


If df = 50 million rows â†’ driver dies.

ğŸ§¯ Fix / Alternative

Use one of these:
```python
df.show()
df.limit(100).display()
df.write.parquet("path")
```
ğŸ¯ Why This Works

Operations remain distributed

No risk of driver overload

### ğŸŸ¨ 3. Anti-Pattern: Iterating Through Rows in Python (Treating Spark Like Pandas)
âŒ Wrong Approach
```python
for row in df.collect():
    process(row)
```
â— Why Itâ€™s Bad

Entire dataset loads to driver
- No parallelism
- Very slow

âœ… Correct Approach (Vectorized Spark Transformations)

```python
df = df.withColumn("new_col", process_udf(df.col))
```

or avoid UDFs entirely and use Spark SQL expressions.

ğŸŸ© 4. Anti-Pattern: Repartitioning Without Reason
âŒ Wrong Approach
df = df.repartition(400)


Dataset is small â†’ creates 400 empty/sparse partitions.

â— Why Bad

- More shuffle
- More tasks
- More small files
- High cost

âœ… Correct Approach
`df = df.repartition(40)`


or let Spark decide.

## ğŸŸª 5. Anti-Pattern: Not Handling Data Skew in Joins
âŒ Example (Wrong)
`df.join(customers, "customer_id")`


If one customer_id = 10 million rows â†’ skew.

ğŸš¨ Symptoms

- Spark stuck at 99%
- One executor runs forever

âœ… Fix 1: Use Broadcast Join (for small table)

```python
from pyspark.sql.functions import broadcast

df = df.join(broadcast(customers), "customer_id")

âœ… Fix 2: Use Salting (for large tables)
from pyspark.sql.functions import rand

df_big = df_big.withColumn("salt", (rand()*10).cast("int"))
df_small = df_small.withColumn("salt", (rand()*10).cast("int"))

df_big.join(df_small, ["customer_id", "salt"])
```

### ğŸŸ« 6. Anti-Pattern: Caching Everything
âŒ Example (Wrong)
df.cache()
df2.cache()
df3.cache()


Developers forget unpersist().

â— Impact

- Executor memory fills
- Jobs fail or spill to disk

âœ… Correct Usage
```python
df.cache()
# ... use df ...
df.unpersist()
```
## ğŸ”¥ What does df.unpersist() do?
## âŒ unpersist() = Remove the DataFrame from Cache

When you write:

`df.unpersist()`


you are telling Spark:

â€œRemove this DataFrame from memory so the RAM can be reused for other jobs.â€

âœ” Internally:

- Spark drops all blocks (cached partitions) from executors
- Frees RAM
- Cleans up storage level associated with the DataFrame
- Caching = only when reused many times.

| Operation        | Meaning                   | Purpose                         |
| ---------------- | ------------------------- | ------------------------------- |
| `df.cache()`     | Store DataFrame in memory | Faster repeated use             |
| `df.unpersist()` | Remove cached data        | Free RAM, avoid memory pressure |

## ğŸ”µ 7. Anti-Pattern: Not Using Predicate Pushdown (Filtering Late)
âŒ Example (Wrong)
```python
df = spark.read.parquet("large_table")
df_filtered = df.filter("country = 'IN'")
```

This reads entire table before filtering.

âœ… Correct (Filter Early)
```python
df = spark.read.parquet("large_table").filter("country = 'IN'")
```
### ğŸŸ£ 8. Anti-Pattern: Too Many Small Files (Delta Lake & Storage Issue)
âŒ Why It Happens

- foreachBatch streaming
- Writing many tiny partitions
- Using repartition(1000) blindly

âŒ Example (Wrong)


`df.write.format("delta").mode("append").save(path)`

If df has 1 row â†’ creates 1 file.
ğŸ§¯ How to Fix It

Option 1: Coalesce output
`df.coalesce(10).write.format("delta").save(path)`

Option 2: Optimize after writes
`OPTIMIZE tableName ZORDER BY (customer_id);`

## ğŸŸ¤ 9. Anti-Pattern: Not Using Delta Lake for Production Data
âŒ Impact of using CSV/Parquet

- No ACID
- No schema validation
- Dirty writes
- ACID conflicts
- Cannot update/delete

ğŸ§¯ Fix

Use:

`df.write.format("delta").save(path)`

### ğŸŸ  10. Anti-Pattern: Hardcoding Paths / Values in Pipeline
âŒ Wrong
`path = "/mnt/raw/2025/12/01/data.parquet"`

Content changes â†’ code must change â†’ fragile pipeline.

ğŸ§¯ Fix

Use:

- Airflow variables
- Databricks widgets
- Config JSON
- Parameterized pipelines
- 

Example:

`dbutils.widgets.get("input_path")`

## ğŸ”´ 11. Anti-Pattern: Using Interactive Clusters for Production
âŒ Problems

- Expensive
- Not auto-terminated
- Manual changes break pipelines

ğŸ§¯ Fix

Use Job Clusters in Databricks.

## ğŸŸ¢ 12. Anti-Pattern: MERGE Without Partitioning (Delta Lake)
âŒ Wrong
`MERGE INTO customers USING updates ON customers.id = updates.id`
Runs slow on billions of rows.

ğŸ§¯ Fix

Partitioned table:
```sql
PARTITION BY country
```

Only merge required partitions.

ğŸŸ¤ 13. Anti-Pattern: No Retry Logic in Pipelines
âŒ Wrong (Airflow)
`PythonOperator(task_id="load", python_callable=load)`

ğŸ§¯ Fix
```python
PythonOperator(
    task_id="load",
    python_callable=load,
    retries=3,
    retry_delay=timedelta(minutes=5)
)
```

ğŸŸ£ 14. Anti-Pattern: Designing Pipelines Without Metadata
âŒ Wrong

50 tables â†’ 50 scripts.

ğŸ§¯ Fix

Use metadata-driven ingestion:

| table | source_path | target_path   |
| ----- | ----------- | ------------- |
| sales | /raw/sales  | /silver/sales |


Pipeline loops through table metadata.

â­ Summary Table of Common PySpark Anti-Patterns
| Anti-Pattern             | Why It Happens              | Impact                | Fix                      |
| ------------------------ | --------------------------- | --------------------- | ------------------------ |
| Python UDFs              | Python habits               | Slow, no optimization | Use Spark SQL            |
| collect() / toPandas()   | Debugging                   | Crash driver          | Use df.show(), limit     |
| Row loops                | Treating Spark as Python    | Very slow             | Use transformations      |
| Unnecessary repartition  | Misunderstanding partitions | Shuffle explosion     | Keep partitions low      |
| Data skew                | Uneven data                 | Long-running tasks    | Broadcast, salting       |
| Over-caching             | Misuse                      | Memory overflow       | Cache selectively        |
| Late filtering           | Not using pushdown          | Full scan             | Filter early             |
| Small files              | Bad write strategy          | Slow reads            | Coalesce, OPTIMIZE       |
| Using CSV/Parquet        | Lack of Delta knowledge     | No ACID               | Use Delta                |
| Hardcoding               | Poor pipeline design        | Non-scalable          | Use parameters           |
| MERGE without partitions | Poor design                 | Expensive             | Partition, CDF           |
| No retry logic           | Oversight                   | Job failures          | Add retries              |
| No metadata layer        | Manual approach             | Large maintenance     | Metadata-driven pipeline |
