{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b5a7100-38fb-4362-a18d-3ef1a5c606b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# step 1: Create a Databricks notebook\n",
    "\n",
    "1. **Go to Workspace** →** Create → Notebook**\n",
    "2. Select:\n",
    "- **Language:** Python\n",
    "- **Cluster:**(Create cluster / use serverless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550b7172-a284-4ae6-a24d-c9836f998e2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+-------------+-----+\n|customer_id|monthly_charge|tenure_months|support_calls|churn|\n+-----------+--------------+-------------+-------------+-----+\n|          1|           899|           24|            1|    0|\n|          2|          1299|            6|            5|    1|\n|          3|           699|           12|            2|    0|\n|          4|          1599|            3|            6|    1|\n|          5|           499|           30|            0|    0|\n+-----------+--------------+-------------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [\n",
    "    (1, 899, 24, 1, 0),\n",
    "    (2, 1299, 6, 5, 1),\n",
    "    (3, 699, 12, 2, 0),\n",
    "    (4, 1599, 3, 6, 1),\n",
    "    (5, 499, 30, 0, 0)\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"monthly_charge\", \"tenure_months\", \"support_calls\", \"churn\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60926c3-8039-4567-8357-6865035cc5fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Vectorize Features (Required by PySpark ML)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|        features|churn|\n+----------------+-----+\n|[899.0,24.0,1.0]|    0|\n|[1299.0,6.0,5.0]|    1|\n|[699.0,12.0,2.0]|    0|\n|[1599.0,3.0,6.0]|    1|\n|[499.0,30.0,0.0]|    0|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"monthly_charge\", \"tenure_months\", \"support_calls\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_ml = assembler.transform(df).select(\"features\", \"churn\")\n",
    "df_ml.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7505de2-0721-4688-90fb-eadf2f1192b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Train/Test Split"
    }
   },
   "outputs": [],
   "source": [
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40af8c9d-da52-4317-a241-cd4f38cfa39f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Train Logistic Regression Model"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"churn\", featuresCol=\"features\")\n",
    "model = lr.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304ee119-3fd8-491c-9f19-3539afcfe5e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Make Predictions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----------+-----------------------------------------+\n|features        |churn|prediction|probability                              |\n+----------------+-----+----------+-----------------------------------------+\n|[1299.0,6.0,5.0]|1    |1.0       |[4.374657798958658E-4,0.9995625342201041]|\n+----------------+-----+----------+-----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"features\", \"churn\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c754830a-491e-4c62-adb5-b00ce4b30c2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Evaluate the Model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    rawPredictionCol=\"prediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC Score:\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "classification example 2025-12-02 21:39:23",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}