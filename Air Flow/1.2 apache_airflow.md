
# Apache Airflow – Beginner to Advanced Notes (with Databricks Integration)

## 1. What is Apache Airflow?

**Short definition**

> **Apache Airflow** is an open-source **workflow orchestration** tool.  
> You define workflows as Python code (called **DAGs**) and Airflow **schedules, runs, and monitors** them.

You use Airflow to:

- Run **ETL / ELT pipelines** on a schedule (hourly, daily, etc.)
- Orchestrate jobs across **multiple systems**: Databricks, Spark, Snowflake, APIs, databases, etc.
- Handle **retries, dependencies, alerts, and backfills** (re-running past dates) in a structured way.

---

## 2. Core Concepts (the 5 words you must know)

1. **DAG (Directed Acyclic Graph)**  
   - Your **workflow**, modeled as a graph with no cycles.  
   - Nodes = tasks, edges = “run B after A”.  
   - Example ID: `daily_sales_etl`.

2. **Task**  
   - A single **step** in the workflow (e.g., “run this Databricks job”, “copy file to ADLS”).

3. **Operator**  
   - A Python class that defines *what* a task does.  
   - Examples:
     - `PythonOperator` – run a Python function.
     - `BashOperator` – run a shell command.
     - `DatabricksRunNowOperator` – trigger a Databricks job.
     - `SimpleHttpOperator` – call a REST API.

4. **Scheduler & Executor**
   - **Scheduler**: reads DAGs, decides *when* to run tasks (according to cron or presets).
   - **Executor**: decides *where/how* tasks run (local, Celery workers, Kubernetes, etc.).

5. **Webserver (UI)**
   - A web UI where you:
     - See DAGs & task status (success, failed, skipped)
     - Trigger runs manually
     - View logs for debugging
     - Manage connections & variables

Everything is defined in **Python**, so Airflow fits naturally into a data engineering environment.

---

## 3. What Software Do I Need to Work with Airflow?

### 3.1 Must-have

- **Python 3.8+**
- **pip** (to run `pip install apache-airflow`)
- A **text editor / IDE** (VS Code, PyCharm, etc.)
- A **database** for Airflow’s metadata:
  - **SQLite** (default; OK for learning)
  - **PostgreSQL** or **MySQL** (recommended for real projects)
- A web browser for the **Airflow UI** (default at `http://localhost:8080`)

### 3.2 Optional but Very Useful

- **Docker** (for quick local setup with `docker-compose`)
- **Git** (version control for your DAGs)
- For Databricks integration:
  - A **Databricks workspace**
  - A **Personal Access Token (PAT)** for Databricks
  - Network access from the Airflow machine → Databricks REST API endpoint

---

## 4. Beginner: Set Up Airflow Locally (Simple “Hello” DAG)

### 4.1 Basic Install (Non-Docker, for Practice)

> Run these in a clean virtual environment if possible.

```bash
# 1. Create & activate virtual environment (Windows PowerShell example)
python -m venv .venv
.\.venv\Scriptsctivate

# 2. Install Airflow with constraints (example version)
pip install "apache-airflow==2.9.0" --constraint   "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.0/constraints-3.8.txt"
```

Optionally set `AIRFLOW_HOME` (if you want a custom folder):

```bash
# Windows (PowerShell)
setx AIRFLOW_HOME "C:irflow"

# Linux/Mac (bash)
export AIRFLOW_HOME=~/airflow
```

Initialize the metadata database & create an admin user:

```bash
airflow db init

airflow users create   --username admin   --firstname Admin   --lastname User   --role Admin   --email admin@example.com   --password admin123
```

Run webserver & scheduler in separate terminals:

```bash
airflow webserver -p 8080
airflow scheduler
```

Now open: **http://localhost:8080** and log in with `admin / admin123`.

---

### 4.2 Simple “Hello Airflow” DAG

Create `AIRFLOW_HOME/dags/hello_airflow.py`:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

def say_hello():
    print("Hello from Airflow!")

default_args = {
    "owner": "gopi",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="hello_airflow",
    start_date=datetime(2025, 1, 1),
    schedule_interval="@daily",   # run once per day
    catchup=False,
    default_args=default_args,
) as dag:

    hello_task = PythonOperator(
        task_id="say_hello_task",
        python_callable=say_hello,
    )
```

Steps:

1. Save the file into the `dags` folder.
2. Refresh the Airflow UI → you should see `hello_airflow` DAG.
3. Turn it **ON** and click **Trigger DAG**.
4. Open the `say_hello_task` → **Log** to see `"Hello from Airflow!"`.

You now understand:

- How Airflow discovers DAGs.
- How tasks run and how to see logs.

---

## 5. How Is Airflow Helpful for Data Engineering with Databricks?

Although Databricks has **Workflows / Jobs**, Airflow is especially useful when you:

1. Need to orchestrate **multiple systems**:
   - REST APIs
   - Databricks (Spark)
   - Snowflake, Redshift, BigQuery, SQL Server, etc.
   - Files in S3/ADLS/GCS
   - Notification tools (Slack, email)
2. Want **central orchestration** for the entire organization, across **multiple Databricks workspaces**.
3. Already use Airflow as a **standard orchestrator** in your company.

Typical Airflow + Databricks data pipeline:

```text
Task 1: Extract data from an API → write raw JSON to ADLS/S3
Task 2: Trigger Databricks job or notebook to do heavy Spark processing
Task 3: Run data-quality checks
Task 4: Load final data into warehouse / Lakehouse table
Task 5: Send Slack/email notification with status
```

Airflow integrates with Databricks using **Databricks Operators** from the `apache-airflow-providers-databricks` package. Under the hood, these operators call the **Databricks REST API**.

---

## 6. Airflow + Databricks: Step-by-Step Example

### 6.1 Goal

> Build a DAG that **once per day**:
> 1. Triggers a **Databricks Job** (already created in the Databricks UI)  
> 2. Waits for completion  
> 3. Logs the result (and potentially notifies via email/Slack)

Assumptions:

- You have a Databricks Job named `daily_sales_transform` with **Job ID = 123**.
- From the Airflow host, you can reach  
  `https://<your-instance>.azuredatabricks.net`.
- You have a **Databricks PAT token**.

---

### 6.2 Create a Databricks Connection in Airflow UI

1. Open Airflow UI → **Admin → Connections**.
2. Click **+** (Add a new connection).
3. Set:
   - **Conn Id**: `databricks_default`
   - **Conn Type**: `Databricks`
   - **Host**: `https://<your-azure-databricks-url>`  
     (Example: `https://adb-1234567890123.4.azuredatabricks.net`)
   - **Login** and **Password**: leave empty.
   - **Extra**:
     ```json
     {
       "token": "YOUR_DATABRICKS_PAT_TOKEN"
     }
     ```
4. Save the connection.

Now any DAG can use Databricks via this `databricks_default` connection.

> In production, the token is usually stored securely (e.g., in a secret backend), not hard-coded.

---

### 6.3 Install Databricks Provider

In your Airflow Python environment:

```bash
pip install apache-airflow-providers-databricks
```

This gives you operators such as:

- `DatabricksRunNowOperator`
- `DatabricksSubmitRunOperator`

---

### 6.4 Airflow DAG to Trigger a Databricks Job Daily

Create `dags/databricks_sales_etl.py`:

```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

default_args = {
    "owner": "data_eng",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=10),
}

with DAG(
    dag_id="databricks_sales_etl",
    default_args=default_args,
    start_date=datetime(2025, 1, 1),
    schedule_interval="0 2 * * *",  # every day at 02:00
    catchup=False,
    tags=["databricks", "sales"],
) as dag:

    start = EmptyOperator(task_id="start")

    run_databricks_job = DatabricksRunNowOperator(
        task_id="run_sales_job",
        databricks_conn_id="databricks_default",
        job_id=123,  # ID of Databricks Job
        notebook_params={           # maps to job parameters in Databricks
            "process_date": "{{ ds }}"   # ds = execution date (YYYY-MM-DD)
        },
    )

    end = EmptyOperator(task_id="end")

    start >> run_databricks_job >> end
```

What this does:

- Every day at **02:00**, Airflow runs `databricks_sales_etl`.
- The `run_sales_job` task calls the Databricks REST API using the connection.
- Databricks runs **Job 123** with a parameter:
  - `process_date = '2025-11-26'` (for example).
- Once the Databricks job finishes, Airflow marks the task as **success** or **failed**.
- You can add email/Slack alerts on failure.

This is already a realistic **production integration** between Airflow and Databricks.

---

## 7. Real-World Pipeline Design: Daily Orders Example

### 7.1 Scenario

> Each night, we:  
> 1. Pull orders from an e-commerce API and save into ADLS as JSON.  
> 2. Trigger a Databricks notebook that:
>    - Reads the JSON  
>    - Writes bronze/silver/gold Delta tables via Unity Catalog  
> 3. Run a data-quality check (row counts, null checks)  
> 4. If checks pass → call another Databricks job to refresh an aggregated “sales mart” for Power BI.  
> 5. Send Slack notification with the status.

### 7.2 DAG Structure

Conceptually:

```text
extract_orders_from_api
        ↓
stage_to_adls
        ↓
run_databricks_raw_to_gold
        ↓
quality_check
   ↙        ↘
fail_notify  refresh_mart_and_notify
```

### 7.3 High-Level Code Sketch

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime, timedelta

def extract_orders(**context):
    # 1) Call REST API
    # 2) Save JSON to local/tmp or directly to ADLS using Azure SDK
    print("Extracting orders for", context["ds"])

def stage_to_adls(**context):
    # Upload the file to ADLS path (e.g., container "raw/orders/{{ ds }}.json")
    print("Staging file to ADLS for", context["ds"])

def quality_check():
    # In real life, connect to Databricks (JDBC/SQL) and check row counts, nulls, etc.
    print("Quality check OK (placeholder)")

default_args = {
    "owner": "data_eng",
    "retries": 1,
    "retry_delay": timedelta(minutes=15),
}

with DAG(
    dag_id="ecommerce_daily_orders_pipeline",
    start_date=datetime(2025, 1, 1),
    schedule_interval="0 1 * * *",  # 1 AM daily
    catchup=False,
    default_args=default_args,
) as dag:

    start = EmptyOperator(task_id="start")

    extract = PythonOperator(
        task_id="extract_orders_from_api",
        python_callable=extract_orders,
        provide_context=True,
    )

    stage = PythonOperator(
        task_id="stage_to_adls",
        python_callable=stage_to_adls,
        provide_context=True,
    )

    # Databricks job: raw JSON -> bronze/silver/gold tables
    run_raw_to_gold = DatabricksRunNowOperator(
        task_id="run_raw_to_gold",
        databricks_conn_id="databricks_default",
        job_id=456,  # Another Databricks Job ID
        notebook_params={
            "process_date": "{{ ds }}",
            "source_container": "raw",
            "target_catalog": "sales",
        },
    )

    # Another Databricks job to refresh sales mart
    refresh_mart = DatabricksRunNowOperator(
        task_id="refresh_sales_mart",
        databricks_conn_id="databricks_default",
        job_id=789,
        notebook_params={
            "process_date": "{{ ds }}",
        },
    )

    quality = PythonOperator(
        task_id="quality_check",
        python_callable=quality_check,
    )

    success = EmptyOperator(task_id="success")
    fail = EmptyOperator(task_id="fail")

    # Trigger rules
    refresh_mart.trigger_rule = TriggerRule.ALL_SUCCESS
    fail.trigger_rule = TriggerRule.ONE_FAILED

    start >> extract >> stage >> run_raw_to_gold >> quality
    quality >> [refresh_mart, fail]
    refresh_mart >> success
```

Teaching points in this example:

- **Jinja templating**: `{{ ds }}` passes the execution date to Databricks.
- **Retries** and **dependencies** are declarative in the DAG.
- Airflow orchestrates **API → ADLS → Databricks → data quality → mart refresh**.
- You can plug in a **SlackOperator** or email on failure.

---

## 8. Intermediate/Advanced Airflow Features (for Data Engineering)

Once your students are comfortable with basic DAGs, you can introduce:

1. **Sensors**
   - Wait for a file or partition to appear:
     - `S3KeySensor`, `WasbBlobSensor`, etc.
   - Example: “Wait until `orders_{{ ds }}.json` arrives in ADLS, then run pipeline.”

2. **Dynamic Task Mapping**
   - Create tasks dynamically based on a list (e.g., one Databricks task per country).
   - Useful for partitioned processing.

3. **XComs (Cross-communication)**
   - Pass small data (IDs, counters) between tasks.
   - Example: one task counts rows and pushes value; another task decides whether to continue based on that count.

4. **Backfilling**
   - Run the DAG for historical dates (e.g., last 30 days of data).
   - You can do this from the UI or CLI.

5. **SLAs & Alerts**
   - Configure SLAs on tasks; if a task is late or fails, send an alert (email, Slack, Teams, etc.).

These features are exactly why people pick Airflow instead of writing custom crons and scripts.

---

## 9. When to Choose Airflow vs Databricks Workflows

**Use Databricks Workflows** when:

- Most or all steps are **inside Databricks** (Delta Live Tables, notebooks, jobs).
- You want a simpler, Databricks-native orchestration layer.
- You don’t need complex cross-system orchestration.

**Use Airflow** when:

- You need to orchestrate **many external systems** in addition to Databricks.
- The organization already standardised on Airflow as central orchestrator.
- You want:
  - Cross-workspace orchestration
  - Multiple cloud services integration (AWS/Azure/GCP)
  - Advanced scheduling, backfills, and custom executors (Celery/Kubernetes).

---

## 10. Short, Trainer-Friendly Answers

You can use these in slides or interviews:

**Q: What is Apache Airflow?**  
> Airflow is an open-source workflow orchestration tool where you define workflows as Python code (DAGs), and it schedules, runs, and monitors tasks with dependencies, retries, and logging.

**Q: What software do I need to work with Airflow?**  
> You need Python, a metadata database (SQLite/Postgres/MySQL), the Airflow webserver and scheduler, and optionally Docker and Git. For Databricks integration, you also need a Databricks workspace and a PAT token.

**Q: How does Airflow help with Databricks?**  
> Airflow can trigger Databricks jobs (via REST API and Databricks operators), pass parameters like process dates, manage retries and dependencies, orchestrate multiple systems around Databricks, and give a single monitoring UI for the entire data pipeline.

**Q: Real-world example?**  
> A daily pipeline where Airflow:
> - Pulls order data from an API, stores it in ADLS  
> - Triggers a Databricks job that converts data to bronze/silver/gold tables  
> - Runs quality checks  
> - Refreshes a sales mart and notifies stakeholders  
> all from a single DAG and UI.

---
