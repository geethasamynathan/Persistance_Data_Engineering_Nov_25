# What Is DLT?
DLT is a declarative framework for building reliable, maintainable and testable data pipelines. It allows data engineers and analysts to define data transformations using SQL (or Python) and automatically manages the underlying infrastructure and data flow.

Delta Live Tables (DLT) is a **Databricks feature** for building **reliable, declarative data pipelines** on top of **Delta Lake**.

Instead of writing manual ETL jobs and managing orchestration yourself, you:

- Declare **what tables should exist** and  
- Define **how they are computed** from other tables or sources  

Databricks then **automatically manages** the pipeline, dependencies, and incremental processing.

## 1. What Are Delta Live Tables?

Delta Live Tables are:

- **Managed pipelines** where each step is a **Delta table** or **view**
- Built using **SQL or Python (PySpark)**  
- Automatically **kept up to date** as new data arrives  
- Designed for **reliable batch and streaming ETL/ELT** 

 In simple terms:  
> **Delta Lake** = how data is stored (format + ACID).  
> **Delta Live Tables** = how data is *continuously built and refreshed* as a pipeline.
>
> ### 2.1 Pipelines, Not Standalone Jobs

In DLT, you create a **pipeline** made of multiple tables:

- `raw_bronze` – raw ingest
- `clean_silver` – cleaned/validated data
- `agg_gold` – business aggregates

You define relationships between them, and DLT figures out the **correct execution order** and **incremental updates**.

---

### 2.2 Live Tables Are Delta Tables

Every DLT “live table” is stored as a **Delta table** in your data lake (S3 / ADLS / GCS).

Benefits:

- ACID transactions  
- Time travel  
- Schema enforcement & evolution  
- Efficient reads (Parquet + Delta log)

---

### 2.3 Declarative Definitions (SQL & Python)

You define tables declaratively.

#### SQL Example

```sql
CREATE OR REFRESH LIVE TABLE raw_orders
AS
SELECT * FROM cloud_files("s3://my-bucket/raw/orders", "json");
```
```sql
CREATE OR REFRESH LIVE TABLE clean_orders
AS
SELECT *
FROM LIVE.raw_orders
WHERE status = 'completed';

```

in python
```python
import dlt
from pyspark.sql.functions import col

@dlt.table
def raw_orders():
    return spark.readStream.format("cloudFiles") \
        .option("cloudFiles.format", "json") \
        .load("s3://my-bucket/raw/orders")

@dlt.table
def clean_orders():
    return dlt.read("raw_orders").where(col("status") == "completed")
```