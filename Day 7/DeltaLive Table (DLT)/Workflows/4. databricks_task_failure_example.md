
# Databricks Workflows â€“ Task Failure Explained (Extended Row Count Example)

## 1. Objective

We extend the previous Databricks workflow example:

- **Task 1 (`count_rows`)** â€“ Reads a CSV file, counts rows, stores values.
- **Task 2 (`print_result`)** â€“ Reads values from Task 1 and prints a summary.

Now we add **Task Failure** concepts:

- What is a Task Failure?
- How can a task fail (natural failures, business rule failures)?
- How to handle failures:
  - Guard conditions
  - Retries
  - Timeouts
  - â€œRun if failedâ€ branches
- A small **alert task** that runs only when something fails.
- A **Community Edition** variant using `dbutils.notebook.run` and `try/except`.

---

## 2. What Is a Task Failure in Databricks?

In a Databricks Job (Workflow):

- A **Job** contains one or more **Tasks**.
- Each **Task run** ends in a status such as:
  - `SUCCESS`
  - `FAILED`
  - `UPSTREAM_FAILED`
  - `SKIPPED`
  - `TIMED_OUT`
  - `CANCELED`

**Task Failure** means the task **did not finish successfully**, usually because:

- Your code raised an exception.
- Spark encountered an error (file not found, schema mismatch, permissions, OOM, etc.).
- The task exceeded its configured **timeout**.
- The run was manually cancelled or the cluster had a fatal issue.

When a task fails:

- That task shows as **FAILED**.
- Any **downstream tasks** that depend on it:
  - May become **UPSTREAM_FAILED / SKIPPED**, or
  - May run only if configured with â€œRun if previous tasks failedâ€.

---

## 3. Base Workflow (Recap)

Weâ€™ll assume the following already exists:

### 3.1. Notebook 1 â€“ `/Jobs/count_rows_by_file` (Producer)

```python
# --------------------------------------------------------
# 1. Define widgets (job parameters)
# --------------------------------------------------------
dbutils.widgets.text("file_name", "", "CSV file name (e.g. DimSalesTerritory.csv)")
dbutils.widgets.text(
    "base_path",
    "/Volumes/trg_catalog/trg_schema/trg_volume",
    "Base folder path"
)

file_name = dbutils.widgets.get("file_name")
base_path = dbutils.widgets.get("base_path")

if not file_name:
    raise ValueError("file_name parameter is empty â€“ please pass a CSV file name.")

# Build full file path
full_path = f"{base_path.rstrip('/')}/{file_name}"
print(f"ğŸ“‚ Reading file: {full_path}")

# --------------------------------------------------------
# 2. Read CSV and count rows
# --------------------------------------------------------
df = (
    spark.read
        .option("header", True)
        .csv(full_path)
)

row_count = df.count()
print(f"âœ… Row count in {file_name}: {row_count}")

# --------------------------------------------------------
# 3. Store values for use by other tasks in the SAME JOB
# --------------------------------------------------------
dbutils.jobs.taskValues.set(key="row_count", value=row_count)
dbutils.jobs.taskValues.set(key="file_name", value=file_name)

print("Row count and file name stored in dbutils.jobs.taskValues")
```

### 3.2. Notebook 2 â€“ `/Jobs/print_row_count_from_job` (Consumer)

```python
# This notebook assumes the first task in the Job is named "count_rows"

row_count = dbutils.jobs.taskValues.get(
    taskKey="count_rows",   # MUST match the producer task name in the Job
    key="row_count"
)

file_name = dbutils.jobs.taskValues.get(
    taskKey="count_rows",
    key="file_name"
)

print("ğŸ“Š Summary from previous task")
print(f"  File name : {file_name}")
print(f"  Row count : {row_count}")
```

### 3.3. Job / Workflow (2 Tasks)

- **Task 1**: `count_rows`
  - Notebook: `/Jobs/count_rows_by_file`
  - Parameters: `file_name`, `base_path`
- **Task 2**: `print_result`
  - Notebook: `/Jobs/print_row_count_from_job`
  - **Depends on**: `count_rows`

Graph:

```text
count_rows  --->  print_result
```

---

## 4. Natural Task Failures (e.g., File Not Found)

### 4.1. Example Failure

If `file_name` is set incorrectly in Task 1, for example:

```text
file_name = "WrongFileName.csv"
```

and the file does NOT exist under:

```text
/Volumes/trg_catalog/trg_schema/trg_volume/WrongFileName.csv
```

then:

```python
df = (
    spark.read
        .option("header", True)
        .csv(full_path)
)
```

will fail with a Spark error. Result:

- Task `count_rows` â†’ **FAILED**
- Task `print_result` â†’ **UPSTREAM_FAILED / SKIPPED**

### 4.2. Safer Version with Guard Check

You can add a **guard** before reading:

```python
# List files in the base path
files = dbutils.fs.ls(base_path)
file_names = [f.name for f in files]

if file_name not in file_names:
    raise FileNotFoundError(f"ğŸš« File '{file_name}' not found in {base_path}")
```

Now:

- If the file is missing, the task fails with a **clear error message**.
- The failure is still visible as **FAILED**, but you understand the reason quickly.

---

## 5. Business Rule Failure (Row Count == 0)

Sometimes the file exists, but data is invalid or insufficient (e.g., empty file).  
You can **intentionally fail the task** if a business rule isnâ€™t met.

### 5.1. Add a Business Rule Check

Extend Notebook 1 after computing `row_count`:

```python
row_count = df.count()
print(f"âœ… Row count in {file_name}: {row_count}")

# Business rule: require at least 1 row
if row_count == 0:
    raise Exception(f"ğŸš« Business rule failure: {file_name} has 0 rows. Failing the task.")

# Only store values if rule passes
dbutils.jobs.taskValues.set(key="row_count", value=row_count)
dbutils.jobs.taskValues.set(key="file_name", value=file_name)

print("Row count and file name stored in dbutils.jobs.taskValues")
```

Effects:

- If file is empty:
  - Task `count_rows` â†’ **FAILED** (on purpose).
  - `taskValues.set` is not executed.
  - Task `print_result` â†’ **UPSTREAM_FAILED / SKIPPED**.
- If file has rows:
  - Workflow behaves normally.

This is an example of **TaskFailure used as data quality enforcement**.

---

## 6. Configuring Failure Behavior in the Task (Retries, Timeouts, Run Conditions)

In the Databricks Job UI, click a task (like `count_rows`). Youâ€™ll see several important settings:

### 6.1. Retries

You can configure:

- **Max retrials** (e.g., 2 or 3)
- **Min retry interval** (e.g., 60 seconds)

Use for **transient issues**, such as:

- Cluster startup/transient failures
- Momentary network problems
- Temporary unavailability of external systems

Example configuration:

- Retries: `2`
- Min retry interval: `60` seconds

If the first attempt fails, Databricks will retry the task up to 2 additional times.

---

### 6.2. Timeout

You can define a **timeout** for each task, e.g., `3600` seconds (1 hour).

- If the task runs longer than this, it will be **terminated**.
- Status becomes **TIMED_OUT** (treated as failure).

Use timeouts to prevent:

- Runaway jobs
- Excessive cluster usage
- Violations of SLAs

---

### 6.3. â€œRun ifâ€ Conditions for Downstream Tasks

Each task can be configured to run:

- **Only if all previous tasks succeed** (default)
- **If one or more previous tasks failed**
- **Always**, regardless of upstream status

This enables patterns like:

- Normal processing branch (success path)
- Failure branch (alerting, cleanup, logging)

---

## 7. Adding a Failure-Handling Task (Alert Branch)

We can add a **third task** that runs **only when `count_rows` fails**.

### 7.1. Notebook 3 â€“ `/Jobs/notify_on_failure`

```python
print("âš ï¸ ETL Failure Notification")

# In a real setup, you might:
# - Send an email notification
# - Call a webhook (Slack/Teams)
# - Insert an entry into an audit/log table

print("A previous task in the Databricks Workflow has failed.")
```

### 7.2. Configure Task 3 in the Job

In the Job â†’ **Tasks** tab:

1. Click **+ Add task**.
2. **Task name**: `notify_failure`
3. **Type**: `Notebook`
4. **Source**: `Workspace`
5. **Path**: `/Jobs/notify_on_failure`
6. **Depends on**: select `count_rows`
7. Look for the setting **â€œRun ifâ€** (or similar):
   - Choose **â€œOne or more previous tasks failedâ€**.

Now the logical structure is:

```text
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> print_result (on success)
count_rows  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> notify_failure (on failure)
```

Behavior:

- If `count_rows` **succeeds**:
  - `print_result` runs.
  - `notify_failure` does **not** run.
- If `count_rows` **fails**:
  - `print_result` is UPSTREAM_FAILED / SKIPPED.
  - `notify_failure` **runs** and logs the failure (or sends a real alert).

This is a standard **alerting / failure-handling pattern**.

---

## 8. What To Do When a Task Fails â€“ Practical Steps

When you see a red (FAILED) task in the Job run UI:

### 8.1. Investigate the Root Cause

1. Click the failed task.
2. Open:
   - **Output** tab â€“ for notebook prints and Python/Spark errors.
   - **Driver logs** â€“ for detailed Spark stack traces.
3. Identify common issues:
   - Incorrect parameter values (wrong file name, base path).
   - Permission problems (no access to Volume/table).
   - Schema errors.
   - External system issues.

### 8.2. Fix and Rerun

After fixing the code / configuration:

- **Option 1** â€“ Run the entire Job again (most common).
- **Option 2** â€“ Use features like â€œRun nowâ€ on a specific task or clone the job run with adjusted parameters if needed.

### 8.3. Design Improvements to Avoid Future Failures

- Add **guard conditions** (check inputs before processing).
- Implement **business rule checks** (e.g., row count > 0).
- Use **retries** for transient issues.
- Set **timeouts** for long-running tasks.
- Make tasks **idempotent**:
  - Re-running a task should not corrupt data (e.g., use MERGE, overwrites with partitions, etc.).
- Add **alert tasks** triggered by failures (â€œRun if failedâ€).

---

## 9. Simulating Task Failure Handling in Databricks Community Edition

In Community Edition (no full Jobs UI), you use:

- `dbutils.notebook.run()` to call notebooks.
- `dbutils.notebook.exit()` to return values.
- `try/except` in an orchestrator notebook to handle failures.

### 9.1. Notebook 1 â€“ return row_count via `exit`

In `/Jobs/count_rows_by_file` (CE variant), at the end:

```python
# After computing row_count and applying any business rules:
if row_count == 0:
    raise Exception(f"ğŸš« Business rule failure: {file_name} has 0 rows.")

dbutils.notebook.exit(str(row_count))
```

### 9.2. Notebook 2 â€“ print summary from parameters

`/Jobs/print_row_count_from_notebook`:

```python
dbutils.widgets.text("file_name", "", "File name")
dbutils.widgets.text("row_count", "", "Row count")

file_name = dbutils.widgets.get("file_name")
row_count = dbutils.widgets.get("row_count")

print("ğŸ“Š Summary from orchestrator")
print(f"  File name : {file_name}")
print(f"  Row count : {row_count}")
```

### 9.3. Notebook 3 â€“ `/Jobs/notify_on_failure` (same as before)

```python
print("âš ï¸ ETL Failure Notification (Community Edition variant)")
```

### 9.4. Orchestrator Notebook with try/except

`/Jobs/run_orchestrated_row_count`:

```python
base_path = "/Volumes/trg_catalog/trg_schema/trg_volume"
file_name = "DimSalesTerritory.csv"

try:
    # 1) Run the first notebook to get row count
    row_count_str = dbutils.notebook.run(
        "/Jobs/count_rows_by_file",
        timeout_seconds=600,
        arguments={
            "file_name": file_name,
            "base_path": base_path
        }
    )

    # 2) If success, run the printer notebook
    dbutils.notebook.run(
        "/Jobs/print_row_count_from_notebook",
        timeout_seconds=300,
        arguments={
            "file_name": file_name,
            "row_count": row_count_str
        }
    )

except Exception as e:
    # Any failure from count_rows_by_file comes here
    print("âš ï¸ Orchestrator caught a failure:", e)

    # 3) Call the failure notification notebook
    dbutils.notebook.run(
        "/Jobs/notify_on_failure",
        timeout_seconds=300,
        arguments={}
    )
```

Behavior:

- If Notebook 1 succeeds:
  - Notebook 2 prints the summary.
- If Notebook 1 fails:
  - The `except` block runs, and `notify_on_failure` is executed.

This is the **notebook-only equivalent** of success/failure branches in Jobs.

---

## 10. Summary â€“ How to Explain Task Failure in Interviews / Training

**Definition:**

> A *Task Failure* in Databricks Workflow is when a step in a Job cannot complete successfully due to errors in code, data, configuration, or environment. Databricks marks the task as FAILED, and downstream tasks either skip or execute depending on configured â€œRun ifâ€ conditions.

**Key practices:**

- Use **guards** and **business rules** to catch problems early.
- Configure **retries** and **timeouts**.
- Separate **success path** and **failure path** using â€œRun if failedâ€ tasks.
- Make tasks **idempotent** and safe to rerun.
- Use **alert tasks** to notify teams of failures.
- In Community Edition, simulate task failures with `try/except` and `dbutils.notebook.run`.

**Sample portfolio bullet:**

> *â€œDesigned Databricks Workflows with robust failure handling: guarding against missing inputs, enforcing data-quality rules (e.g., non-empty files), configuring retries and timeouts, and routing failures to a dedicated alert task that runs only on upstream failures. Implemented an equivalent notebook-based failure-handling pattern in Databricks Community Edition using `dbutils.notebook.run` and exception handling.â€*
