
# Databricks Job with Parameter (Filename) â€“ Row Count Example

## 1. Scenario

You have multiple CSV files in a folder (Volume or FileStore) and you want a **Databricks Job (Workflow)** that:

- Takes **file name as a parameter**
- Reads that specific CSV file
- Counts the number of rows
- Prints the result (and optionally stores it for use by other tasks)

This note shows:

- A **parameterised notebook** using widgets
- How to **create a Job** that passes parameters
- How to **reuse the same job** for different files
- How to **simulate this in Community Edition**

---

## 2. Folder Structure for CSV Files

Example with a Unity Catalog Volume:

```text
/Volumes/trg_catalog/trg_schema/trg_volume/
    â”œâ”€â”€ Customers.csv
    â”œâ”€â”€ Orders.csv
    â””â”€â”€ DimSalesTerritory.csv
```

> If you are using `/FileStore/tables/...` or any other mount point, adjust the base path accordingly.

---

## 3. Create a Parameterised Notebook

Create a new notebook, e.g.:

```text
/Jobs/count_rows_by_file
```

Paste this code:

```python
# --------------------------------------------------------
# 1. Define widgets (job parameters)
# --------------------------------------------------------
dbutils.widgets.text("file_name", "", "CSV file name (e.g. DimSalesTerritory.csv)")
dbutils.widgets.text(
    "base_path",
    "/Volumes/trg_catalog/trg_schema/trg_volume",
    "Base folder path"
)

file_name = dbutils.widgets.get("file_name")
base_path = dbutils.widgets.get("base_path")

if not file_name:
    raise ValueError("file_name parameter is empty â€“ please pass a CSV file name.")

# Build the full path to the file
full_path = f"{base_path.rstrip('/')}/{file_name}"

print(f"ğŸ“‚ Reading file: {full_path}")

# --------------------------------------------------------
# 2. Read CSV and count rows
# --------------------------------------------------------
df = (
    spark.read
        .option("header", True)
        .csv(full_path)
)

row_count = df.count()
print(f"âœ… Row count in {file_name}: {row_count}")

# --------------------------------------------------------
# 3. (Optional) Expose value to other tasks in the Job
# --------------------------------------------------------
try:
    dbutils.jobs.taskValues.set(key="row_count", value=row_count)
    print("Row count stored in dbutils.jobs.taskValues with key = 'row_count'")
except Exception as e:
    # Will fail here if not running as a job â€“ safe to ignore while testing in notebook
    print(f"(Info) Could not set taskValues (probably not running as a job): {e}")
```

### 3.1. Test the notebook manually

1. In the notebook menu, click **`Widgets` â†’ `Show`** if widgets are hidden.
2. Set:
   - `file_name` = `DimSalesTerritory.csv`
   - `base_path` = `/Volumes/trg_catalog/trg_schema/trg_volume`
3. Run **all cells**.
4. You should see output similar to:

```text
ğŸ“‚ Reading file: /Volumes/trg_catalog/trg_schema/trg_volume/DimSalesTerritory.csv
âœ… Row count in DimSalesTerritory.csv: 12345
(Info) Could not set taskValues (probably not running as a job): ...
```

---

## 4. Create a Databricks Job with Parameters (Jobs & Pipelines)

> This is for a **full Databricks workspace** (trial/paid) where you have the **Jobs & Pipelines** menu.  
> The job will call the notebook and pass the `file_name` and `base_path` parameters.

### 4.1. Open the Jobs screen

1. Left sidebar â†’ **Jobs & Pipelines**.
2. Click **Create job**.

### 4.2. Configure the task

In the **Tasks** tab:

1. **Job name**: `count_rows_job`
2. Under **Tasks**, click **`+ Add task`**.
3. Fill in:
   - **Task name**: `count_rows`
   - **Type**: `Notebook`
   - **Source**: `Workspace`
   - **Path**: select `/Jobs/count_rows_by_file` (or your notebook path)
   - **Compute**: choose an existing cluster or Serverless (if available)
4. Scroll down to **Parameters** (or **Base parameters / Advanced**, depending on UI).
5. Add these two parameters:

   | Name       | Value                                             |
   |------------|---------------------------------------------------|
   | file_name  | `DimSalesTerritory.csv`                           |
   | base_path  | `/Volumes/trg_catalog/trg_schema/trg_volume`      |

   These names **must match exactly** the widget names in the notebook.

6. Click **Create task** (or **Save task**).

### 4.3. Run the job

1. On the job page, top-right â†’ click **Run now**.
2. After the run finishes, click the **latest run**.
3. Click the `count_rows` task â†’ open **Output / Logs**.

You should see something like:

```text
ğŸ“‚ Reading file: /Volumes/trg_catalog/trg_schema/trg_volume/DimSalesTerritory.csv
âœ… Row count in DimSalesTerritory.csv: 12345
Row count stored in dbutils.jobs.taskValues with key = 'row_count'
```

---

## 5. Run the Job for Different Files

You do **not** need a new job for each file. Instead, reuse the same job and only change the parameter.

There are two options:

### 5.1. Edit task parameters and run

1. Open the job â†’ **Tasks** â†’ click on `count_rows`.
2. Change the `file_name` parameter value to:

   - `Customers.csv`  
   - or `Orders.csv`

3. Click **Save**.
4. Click **Run now**.

### 5.2. Use â€œRun with different parametersâ€ (if available)

In newer UIs:

1. Open the job â†’ **Runs** tab.
2. Click **Run now â†’ With different parameters**.
3. Edit `file_name` to the desired CSV file.
4. Run the job.

Each run now processes a different file but uses the same notebook and job definition.

---

## 6. Simulating a Parameterised Job in Databricks Community Edition

Databricks **Community Edition** does not provide the full Jobs / Workflows interface, but you can still:

- Use **widgets** for parameters.
- Call notebooks from another notebook using `dbutils.notebook.run`.

### 6.1. Use the same parameterised notebook

Re-use `/Jobs/count_rows_by_file` exactly as written in section 3.

### 6.2. Orchestrator notebook example (simulated job)

Create another notebook, e.g. `/Jobs/run_count_for_files`, with:

```python
files_to_process = [
    "DimSalesTerritory.csv",
    "Customers.csv",
    "Orders.csv",
]

base_path = "/Volumes/trg_catalog/trg_schema/trg_volume"

for file_name in files_to_process:
    print(f"=== Processing {file_name} ===")
    result = dbutils.notebook.run(
        "/Jobs/count_rows_by_file",     # notebook path
        timeout_seconds=600,
        arguments={
            "file_name": file_name,
            "base_path": base_path
        }
    )
    print("Notebook finished with result:", result)
    print()
```

Run this orchestrator notebook:

- It sequentially calls the `count_rows_by_file` notebook.
- Passes different `file_name` values as **parameters**.
- Prints output for each file.

> This is a **good way to teach parameters and orchestration** in Community Edition when the full Jobs UI is not available.

---

## 7. Key Teaching Points / Summary

- Use **`dbutils.widgets.text()`** to define parameters in notebooks.
- Retrieve parameter values with **`dbutils.widgets.get()`**.
- Build dynamic paths like `f"{base_path}/{file_name}"` to read different files.
- In a **full workspace**:
  - Use **Jobs & Pipelines â†’ Create job â†’ Add task**.
  - Map **job parameters â†’ notebook widgets**.
- In **Community Edition**:
  - Simulate jobs by using `dbutils.notebook.run()` from an orchestrator notebook.

This example is perfect for a portfolio or training note:

> *â€œImplemented a parameterised Databricks job that takes a CSV filename as input, reads data from a Unity Catalog Volume, and computes row counts dynamically for different files. Demonstrated both Jobs-based orchestration and notebook-driven orchestration in Community Edition.â€*
