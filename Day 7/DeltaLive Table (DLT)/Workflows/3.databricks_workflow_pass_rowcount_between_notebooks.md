
# Databricks Workflow: Pass Row Count from One Notebook to Another

## 1. Objective

We extend the previous example:

> A Databricks job that takes **CSV file name as a parameter**,  
> counts the rows in that file, and then **passes the row count** to a second notebook, which prints a summary.

We will build:

- **Notebook 1** â€“ Parameterised, reads CSV, counts rows, stores values via `dbutils.jobs.taskValues.set`.
- **Notebook 2** â€“ Reads values from the previous task via `dbutils.jobs.taskValues.get` and prints them.
- **Job (Workflow)** â€“ With **two tasks** in sequence: `count_rows` â†’ `print_result`.

At the end, youâ€™ll also see how to do something similar in **Databricks Community Edition** using only `dbutils.notebook.run`.

---

## 2. Assumptions

- CSV files are stored in a folder (for example, a Unity Catalog Volume):

```text
/Volumes/trg_catalog/trg_schema/trg_volume/
    â”œâ”€â”€ Customers.csv
    â”œâ”€â”€ Orders.csv
    â””â”€â”€ DimSalesTerritory.csv
```

- You already have a working cluster or Serverless compute.
- You are using a workspace that has **Jobs & Pipelines** (Workflows).

---

## 3. Notebook 1 â€“ Count Rows and Store Values (Producer)

Create a notebook, for example:

```text
/Jobs/count_rows_by_file
```

Paste this code:

```python
# --------------------------------------------------------
# 1. Define widgets (job parameters)
# --------------------------------------------------------
dbutils.widgets.text("file_name", "", "CSV file name (e.g. DimSalesTerritory.csv)")
dbutils.widgets.text(
    "base_path",
    "/Volumes/trg_catalog/trg_schema/trg_volume",
    "Base folder path"
)

file_name = dbutils.widgets.get("file_name")
base_path = dbutils.widgets.get("base_path")

if not file_name:
    raise ValueError("file_name parameter is empty â€“ please pass a CSV file name.")

# Build full file path
full_path = f"{base_path.rstrip('/')}/{file_name}"

print(f"ğŸ“‚ Reading file: {full_path}")

# --------------------------------------------------------
# 2. Read CSV and count rows
# --------------------------------------------------------
df = (
    spark.read
        .option("header", True)
        .csv(full_path)
)

row_count = df.count()
print(f"âœ… Row count in {file_name}: {row_count}")

# --------------------------------------------------------
# 3. Store values for use by other tasks in the SAME JOB
# --------------------------------------------------------
dbutils.jobs.taskValues.set(key="row_count", value=row_count)
dbutils.jobs.taskValues.set(key="file_name", value=file_name)

print("Row count and file name stored in dbutils.jobs.taskValues")
```

### 3.1. What this notebook does

- Reads two parameters (widgets): `file_name` and `base_path`.
- Builds a path like `/Volumes/.../DimSalesTerritory.csv`.
- Reads the CSV with Spark.
- Counts the number of rows.
- Stores:
  - `row_count` under key `"row_count"`
  - `file_name` under key `"file_name"`
- These values are stored **inside the Job run context** and can be retrieved by other tasks in the same Job.

---

## 4. Notebook 2 â€“ Read Values from Previous Task (Consumer)

Create a second notebook, for example:

```text
/Jobs/print_row_count_from_job
```

Paste this code:

```python
# This notebook assumes the first task in the Job is named "count_rows"

row_count = dbutils.jobs.taskValues.get(
    taskKey="count_rows",   # MUST match the producer task name in the Job
    key="row_count"
)

file_name = dbutils.jobs.taskValues.get(
    taskKey="count_rows",
    key="file_name"
)

print("ğŸ“Š Summary from previous task")
print(f"  File name : {file_name}")
print(f"  Row count : {row_count}")
```

### 4.1. What this notebook does

- Uses `dbutils.jobs.taskValues.get` to fetch values **produced in another task** in the same Job run.
- `taskKey="count_rows"` must match the **Task name** of Notebook 1 in the Job.
- It prints a clean summary that you can show to students or log for debugging.

---

## 5. Create the Databricks Job (Workflow)

### 5.1. Open Jobs & Pipelines

1. In the left sidebar, click **Jobs & Pipelines**.
2. Click **Create job**.

### 5.2. Configure Task 1 â€“ `count_rows` (Producer)

In the **Tasks** tab:

1. **Job name**: `row_count_demo` (or any name).
2. Click **+ Add task**.
3. Fill in:
   - **Task name**: `count_rows`  
     > This name must match `taskKey="count_rows"` in Notebook 2.
   - **Type**: `Notebook`
   - **Source**: `Workspace`
   - **Path**: `/Jobs/count_rows_by_file`
   - **Compute**: choose a cluster / serverless.
4. Scroll down to **Parameters** (Base parameters).
5. Add:

   | Name      | Value                                             |
   |-----------|---------------------------------------------------|
   | file_name | `DimSalesTerritory.csv`                           |
   | base_path | `/Volumes/trg_catalog/trg_schema/trg_volume`      |

6. Click **Create task** / **Save task**.

### 5.3. Configure Task 2 â€“ `print_result` (Consumer)

1. Still in the same Job â†’ **Tasks** tab.
2. Click **+ Add task**.
3. Fill in:
   - **Task name**: `print_result`
   - **Type**: `Notebook`
   - **Source**: `Workspace`
   - **Path**: `/Jobs/print_row_count_from_job`
   - **Compute**: same cluster or job cluster.
4. Under **Depends on**, select **`count_rows`**.
5. Save the task.

Now the Job graph should look like:

```text
count_rows  --->  print_result
```

---

## 6. Run the Job and View Outputs

1. On the Job page, click **Run now**.
2. Wait until the run status is **Succeeded**.
3. Click the latest run to open the **Run details**.
4. Click on **`count_rows`**:
   - In **Output / Logs**, you should see:

     ```text
     ğŸ“‚ Reading file: /Volumes/trg_catalog/trg_schema/trg_volume/DimSalesTerritory.csv
     âœ… Row count in DimSalesTerritory.csv: 12345
     Row count and file name stored in dbutils.jobs.taskValues
     ```

5. Go back and click **`print_result`**:
   - In **Output / Logs**, you should see:

     ```text
     ğŸ“Š Summary from previous task
       File name : DimSalesTerritory.csv
       Row count : 12345
     ```

This confirms that:

- Notebook 1 computed `row_count` and stored it.
- Notebook 2 successfully read the value from the previous task within the Job.

---

## 7. Optional: Repeat for Different Files

You can **reuse** the same Job and tasks for other CSV files by changing just one parameter.

### 7.1. Change the parameter in Task 1

1. Open the Job â†’ **Tasks** tab.
2. Click on the **`count_rows`** task.
3. Change `file_name` to:
   - `Customers.csv` or `Orders.csv`
4. Save the task.
5. Click **Run now** again.

Notebook 2 will automatically print the new file name and row count for the new file.

---

## 8. Variant for Databricks Community Edition (No Jobs UI)

In **Databricks Community Edition**, you donâ€™t have the full Jobs UI.  
You can still chain notebooks and pass values using **`dbutils.notebook.run`**.

### 8.1. Adjust Notebook 1 to return the count

In `/Jobs/count_rows_by_file` (for CE), you can **exit with the row count**:

```python
# ... same code to compute row_count ...

dbutils.notebook.exit(str(row_count))  # Return row_count as a string
```

### 8.2. Notebook 2 â€“ Use widgets (parameters)

Create `/Jobs/print_row_count_from_notebook`:

```python
dbutils.widgets.text("file_name", "", "File name")
dbutils.widgets.text("row_count", "", "Row count")

file_name = dbutils.widgets.get("file_name")
row_count = dbutils.widgets.get("row_count")

print("ğŸ“Š Summary from orchestrator")
print(f"  File name : {file_name}")
print(f"  Row count : {row_count}")
```

### 8.3. Orchestrator notebook â€“ call both notebooks

Create `/Jobs/run_orchestrated_row_count`:

```python
base_path = "/Volumes/trg_catalog/trg_schema/trg_volume"
file_name = "DimSalesTerritory.csv"

# 1) Run the first notebook to get the row count
row_count_str = dbutils.notebook.run(
    "/Jobs/count_rows_by_file",
    timeout_seconds=600,
    arguments={
        "file_name": file_name,
        "base_path": base_path
    }
)

# 2) Run the second notebook to print the summary
dbutils.notebook.run(
    "/Jobs/print_row_count_from_notebook",
    timeout_seconds=600,
    arguments={
        "file_name": file_name,
        "row_count": row_count_str
    }
)
```

When you run the **orchestrator notebook**:

- It calls Notebook 1, which returns the `row_count` via `dbutils.notebook.exit`.
- It then calls Notebook 2, passing both the `file_name` and `row_count`.
- The output of Notebook 2 shows the summary (similar to the Job-based approach).

---

## 9. Summary & Talking Points for Training / Portfolio

- **Databricks Workflows (Jobs)** can pass values between tasks using `dbutils.jobs.taskValues.set` and `.get`.
- In this example:
  - Task 1 (Notebook 1) reads a parameter (`file_name`), processes a CSV, and sets `row_count` & `file_name` into `taskValues`.
  - Task 2 (Notebook 2) reads those values using `taskKey` and prints a summary.
- For **Community Edition**, similar orchestration is done via `dbutils.notebook.run` and `dbutils.notebook.exit`.
- This scenario is realistic for:
  - Quality checks (row counts, null counts)
  - Audit logging
  - Passing intermediate metrics from one ETL step to another.

Example portfolio line:

> *â€œDesigned a Databricks Workflow that parameterises CSV file processing, computes row counts per file, and shares results between tasks using `dbutils.jobs.taskValues`. Also implemented a Community Edition variant using `dbutils.notebook.run` and `dbutils.notebook.exit` for notebook-level orchestration.â€*
