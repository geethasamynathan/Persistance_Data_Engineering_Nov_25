
# Step-by-step: Databricks Job with Two Tasks

## üîπ Task 1 ‚Äì Producer (sets `record_count`)

**Task name**

- Use a simple name like: `task_1`  
- ‚ö†Ô∏è If you use `task_1` here, it **must** match `taskKey="task_1"` in your second notebook.

**Configuration**

- **Type:** `Notebook`
- **Source:** `Workspace`
- **Path:**
  - Click **Select Notebook**
  - Choose your first notebook (the one with `SELECT COUNT` and:
    ```python
    dbutils.jobs.taskValues.set(key="record_count", value=row_count)
    ```

- **Compute:**
  - Choose an existing cluster **or** keep **Serverless** if it‚Äôs available.

- Click **Create task** at the bottom.

‚úÖ That finishes **Task 1** inside this job.

---

## üîπ Task 2 ‚Äì Consumer (reads `record_count`)

After **Task 1** is created:

1. In the same **Tasks** tab, click **+ (Add task)** below or beside the first box.
2. Set the following:

   - **Task name:** `task_2`
   - **Type:** `Notebook`
   - **Source:** `Workspace`
   - **Path:**  
     Pick your second notebook (the one with):
     ```python
     count_value = dbutils.jobs.taskValues.get(taskKey="task_1", key="record_count")
     print("Count of rows in the table : ", count_value)
     ```

   - **Depends on:** select `task_1`.
   - **Compute:** choose your cluster or Serverless.
3. Click **Create task**.

---

## ‚ñ∂Ô∏è Run the Job

1. Top-right of the Job page ‚Üí click **Run now**.
2. Open the latest run.
3. Click **task_2** ‚Üí **View logs**.

You should see an output similar to:

```text
Count of rows in the table :  <some number>
```
