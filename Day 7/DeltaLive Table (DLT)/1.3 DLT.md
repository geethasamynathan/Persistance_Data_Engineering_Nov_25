![alt text](image-19.png)

in sql editor
create table orders

```sql
CREATE TABLE orders
(
    order_id INT,
    order_date DATE,
    customer_id INT,
    order_status STRING
);

INSERT INTO orders
VALUES
    (1, '2020-01-01', 1, 'COMPLETE'),
    (2, '2020-01-02', 2, 'PENDING'),
    (3, '2020-01-03', 3, 'COMPLETE'),
    (4, '2020-01-04', 4, 'CANCELLED'),
    (5, '2020-01-05', 5, 'COMPLETE');
```

![alt text](image-21.png)
![alt text](image-20.png)

Each folder has a **specific purpose**.  
Below is a detailed explanation.

---

## üü© 1. `DLT_Root/` ‚Äî Main Project Folder

This is the **root directory** of your entire Delta Live Tables pipeline.

It contains:

- Notebooks  
- Python transformation scripts  
- Utility modules  
- Configuration files  
- Documentation (`README.md`)

Think of this as the **top-level project folder**, similar to the root of a Git repository.

### üìå When you run the pipeline, Databricks scans this folder for:

- Transformation logic  
- Exploration notebooks  
- Utility scripts  
- Any Python packages you import  

---

## üü• 2. `explorations/` ‚Äî Data Exploration Notebooks

This folder contains notebooks used for **exploration and testing**, *not* for production pipeline execution.

### You use this folder for:

- Trying out data samples  
- Writing trial SQL queries  
- Understanding schemas  
- Prototyping logic before converting it to a DLT table  
- Debugging transformations  

### üìå Important  
Files inside `explorations/` **do not run as part of the DLT pipeline**.

---

## üü• 3. `transformations/` ‚Äî Pipeline Transformation Scripts

This is the **most important folder** in a DLT project.

Everything inside **becomes part of the actual pipeline**.

Common files:

- `sample_trips_aug_1_1306.py`  
- `sample_zones_aug_1_1306.py`

These files typically define DLT logic such as:

```python
@dlt.table
def bronze_table():
    ...
```

![alt text](image-22.png)
![alt text](image-23.png)
# üì° Understanding Streaming Data (Based on the Picture)

Your diagram shows the basic idea of **streaming ETL** (Extract ‚Üí Transform ‚Üí Load) in a continuous data pipeline.

---

# üü© 1. Source (Green Box)

This represents the **incoming data** that arrives continuously.

### Examples of streaming sources:
- Kafka topic  
- Event Hub / Kinesis  
- Delta table with streaming updates  
- Folder where new files keep landing (JSON, CSV, Parquet‚Ä¶)  
- IoT device data  
- Website clickstream events  

### ‚úî Key points:
- Data arrives **incrementally**, not all at once  
- New records continuously appear  
- Databricks reads this using:

```python
spark.readStream.format(...)
## üîµ 2. Streaming Table (Blue Box ‚Äì Center)

This represents the **real-time processing layer** in a streaming pipeline.

A streaming table:

- Continuously reads new incoming data  
- Processes it as soon as it arrives  
- Forwards the processed data to downstream tables  
- Acts as the ‚Äúlive engine‚Äù of your pipeline  

---

### üß± Delta Live Tables (DLT) Example

```python
@dlt.table(
    comment="A streaming table"
)
def streaming_table():
    return (
        spark.readStream.format("cloudFiles")
            .option("cloudFiles.format", "json")
            .load("/mnt/source")
    )
```
## üìù Summary

A Streaming Table is a continuously running, real-time processing engine that ingests new data as it arrives, transforms it, and outputs it instantly ‚Äî ideal for dashboards, fraud detection, IoT, logs, and any real-time analytics use case.
----

![alt text](image-24.png)

![alt text](image-25.png)

## ‚≠ê 1. What is a View?

A **View** in Databricks is a **logical / virtual table**.

- It **does not store data**
- It runs the query **every time** you read it
- Stored only as metadata in the metastore

### üîπ Example

```sql
CREATE VIEW high_value_orders AS
SELECT * FROM orders
WHERE amount > 1000;
```

Querying the view:

```sql
SELECT * FROM high_value_orders;
```

Databricks re-runs the underlying SELECT each time.

---




# Demo 

![alt text](image-26.png)

```python
import dlt

# Create Streaming Table
import dlt

@dlt.table(
    name="first_stream_table"
)
def first_stream_table():
    df = spark.readStream.table("dlt_catalog.dlt_schema.orders")
    return df

```

Dry Run 
![alt text](image-27.png)
It will create graph.
![alt text](image-28.png)

Run Pipeline
it will create table

## ‚≠ê 2. What is a Materialized View?

A **Materialized View (MV)** is a **pre-computed physical table**.

- Stores results on disk  
- Faster performance for repeated queries  
- Supports **incremental refresh**  
- Ideal for BI dashboards  

### üîπ Example

```python
# Create Materialized view

@dlt.table(
    name="first_mat_view"
)

def first_mat_view():
    df=spark.read.table("dlt_catalog.dlt_schema.orders")
    return df
```
Run pipeline.

Previously Both Streaming table and mat view had output 5 rows. 
![alt text](image-38.png)

Now streaming table has output 2 
whereas mat_view has output 7 rows
```sql
CREATE MATERIALIZED VIEW mv_daily_sales AS
SELECT date(order_date) AS order_day,
       SUM(amount) AS total_sales
FROM orders
GROUP BY date(order_date);
```
![alt text](image-29.png)
---

![alt text](image-30.png)
## ‚≠ê 3. What is a Batch View?

A **Batch View** is produced using **batch processing**, not real-time.

- Processes full or incremental batches  
- Runs on a schedule (e.g., hourly/daily)  
- Stored as a physical table  
- Typical for ETL Silver/Gold layers  

### üîπ Example (DLT Batch Table)
```python
# create batch view
@dlt.table(
    name="first_batch_view"
)
def first_batch_view():
    df=spark.readStream.table("dlt_catalog.dlt_schema.orders")
    return df
```

```python
import dlt

@dlt.table
def silver_orders():
    return spark.table("bronze_orders").filter("order_status != 'CANCELLED'")
```
---
if you want you can rename the file as 1_core_components.py


Add new file as 2_dependency.py in demo folder (source folder)
![alt text](image-34.png)
![alt text](image-31.png)
Materialized view is batch processing .It will read all data
![alt text](image-32.png)

to check that we need to **run pipeline.**
![alt text](image-35.png)

Add new file as 2_dependency.py in demo folder (Demo or source folder)
```python
import dlt
'''creating an  End-To-End Base Pipeline'''

#staging area
@dlt.table(
    name="staging_orders"
)
def staging_orders():
    df=spark.readStream.table("dlt_catalog.dlt_schema.orders")
    return df

# Creating Transformed Area
@dlt.view(
    name="transformed_orders"
)
def transformed_orders():
    df=spark.readStream.table("LIVE.staging_orders")
    return df
    
```
Now do Dry Run.

While you run  all code will run even in first file. Make sure you don't want you can comment the code in first (1_Core_Component.py) file.

Dry run actually not creating table in permenant storage. But your transformed data was created from  LIVE table staging_orders using LIVE.staging_orders



![alt text](image-39.png)


If you want to run any python code with in pipeline.
Goto  Exploration Folder -> open Sample Exploration file there you can write everything and run.

Your notebooks which is explorations does not run when you are running pipeline.  as it is not in source folder

``` python
import dlt

from pyspark.sql.functions import *
'''creating an  End-To-End Base Pipeline'''

#staging area
@dlt.table(
    name="staging_orders"
)
def staging_orders():
    df=spark.readStream.table("dlt_catalog.dlt_schema.orders")
    return df

# Creating Transformed Area
@dlt.view(
    name="transformed_orders"
)
def transformed_orders():
    df=spark.readStream.table("LIVE.staging_orders")
    df=df.withColumn("order_status",lower(col("order_status")))
    return df
    

# Creating aggregated Area
@dlt.table(
    name="aggregated_orders"
)

def aggregated_orders():
    df=spark.readStream.table("transformed_orders")
    df=df.groupBy("order_status").count()
    return df


```
Dry Run

![alt text](image-40.png)

Here in the above code we have not specified LIVE.transformed_orders
still it is mapping. 

That is called Lineage

## ‚≠ê 4. What is a Streaming View?

A **Streaming View** is built using **Spark Structured Streaming** or **DLT Streaming Tables**.

- Continuously updates  
- Processes data in micro-batches  
- Handles late/out-of-order events  
- Supports exactly-once processing  
- 
Add new file as 2_dependency.py in demo folder (source folder)

![alt text](image-37.png)
### üîπ Example

```python
@dlt.table(
  table_properties={"quality": "bronze"},
  comment="Streaming ingestion table"
)
def bronze_orders_stream():
    return spark.readStream.format("cloudFiles")            .option("cloudFiles.format", "json")            .load("/mnt/source/orders")
```

---
## üìå Final Summary Table

| Type | Stores Data? | Updates | Use Case |
|------|--------------|---------|----------|
| **View** | ‚ùå No | Every query | Lightweight logic |
| **Materialized View** | ‚úî Yes | Incremental | Dashboards |
| **Batch View** | ‚úî Yes | Scheduled batch | ETL Silver/Gold |
| **Streaming View** | ‚úî Yes | Real-time | IoT, logs, events |

-----





